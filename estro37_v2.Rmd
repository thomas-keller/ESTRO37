---
title: "estrov2"
author: "Thomas E. Keller"
date: "April 23, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Retweeting tweets from ESTRO37

The current new hotness for retweeting and manipulating twitter data in the R world is rtweet, which is what we'll use today.

The basic outline of code and analysis will be a mishmash of Mike Kearney's [Ruser2018 analysis](https://github.com/mkearney/rstudioconf_tweets) and my old NIPS2016 analysis. Kearney's analysis is quite good, I recommend it for anyone that is interested in methodology.

The network vis also samples from a notebook from a cool biologist Kenneth Turner, check out his [notebook](https://khturner.shinya pps.io/HashtagISME16/)

He (Kearney)'s the author of rtweet, so often there's a bit more of the details sprinkled throughout the analysis.

Before beginning, I'll sprinkle in some of the highly retweets from the first couple days.

First, a good tweet:

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Follow our <a href="https://twitter.com/hashtag/ESTRO37?src=hash&amp;ref_src=twsrc%5Etfw">#ESTRO37</a> Ambassadors, the best way to be informed of what’s going on in any track, whether you are in Barcelona or not! <a href="https://t.co/r73mXWHPi5">pic.twitter.com/r73mXWHPi5</a></p>&mdash; ESTRO (@ESTRO_RT) <a href="https://twitter.com/ESTRO_RT/status/986529229666955264?ref_src=twsrc%5Etfw">April 18, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


Then, a curious tweet (well, I think conferences that still try to lock down taking pictures are fighting against a river at this point):

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Follow our <a href="https://twitter.com/hashtag/ESTRO37?src=hash&amp;ref_src=twsrc%5Etfw">#ESTRO37</a> Ambassadors, the best way to be informed of what’s going on in any track, whether you are in Barcelona or not! <a href="https://t.co/r73mXWHPi5">pic.twitter.com/r73mXWHPi5</a></p>&mdash; ESTRO (@ESTRO_RT) <a href="https://twitter.com/ESTRO_RT/status/986529229666955264?ref_src=twsrc%5Etfw">April 18, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



```{r retrieve}
library(rtweet)

searchfield <-c("ESTRO37")

if (file.exists(file.path("data", "search.rds"))) {
  since_id <- readRDS(file.path("data", "search.rds"))
  since_id <- since_id$status_id[1]
} else {
  since_id <- NULL
}

## search for up to 100,000 tweets mentionging rstudio::conf
rt <- search_tweets(
  paste(searchfield, collapse = " OR "),
  n = 1e5, verbose = FALSE,
  since_id = since_id,
  retryonratelimit = TRUE
)

## if there's already a search data file saved, then read it in,
## drop the duplicates, and then update the `rt` data object
if (file.exists(file.path("data", "search.rds"))) {

  ## bind rows (for tweets AND users data)
  rt <- do_call_rbind(
    list(rt, readRDS(file.path("data", "search.rds"))))

  ## determine whether each observation has a unique status ID
  kp <- !duplicated(rt$status_id)

  ## only keep rows (observations) with unique status IDs
  users <- users_data(rt)[kp, ]

  ## the rows of users should correspond with the tweets
  rt <- rt[kp, ]

  ## restore as users attribute
  attr(rt, "users") <- users
}

## save the data
saveRDS(rt, file.path("data", "search.rds"))

## save shareable data (only status_ids)
saveRDS(rt[, "status_id"], file.path("data", "search-ids.rds"))



```

# Initial vis

Time series of the data in two hour chunks. One REALLY nice thing about rtweet is that it makes plotting the timeseries of tweet a completely lazy-person's function, ts_plot, where you can feed it the aggregation time to summarize over. Here, we go with 2 hour's as that seems like a good medium to begin with.

```{r initvis}
suppressPackageStartupMessages(library(tidyverse))
library(cowplot)
rt %>%
  filter(created_at > "2018-01-29") %>%
  ts_plot("2 hours", color = "transparent") +
  geom_smooth(method = "loess", se = FALSE, span = .1,
  size = 2, colour = "#0066aa") +
  geom_point(size = 5,
    shape = 21, fill = "#ADFF2F99", colour = "#000000dd") +
  theme(axis.text = element_text(colour = "#222222"),
        text=element_text('Roboto Condensed'),
    plot.title = element_text(size = rel(1.7), face = "bold"),
    plot.subtitle = element_text(size = rel(1.3)),
    plot.caption = element_text(colour = "#444444")) +
  labs(title = "Frequency of tweets about ESTRO37 over time",
    subtitle = "Twitter status counts aggregated using two-hour intervals",
    caption = "\n\nSource: Data gathered via Twitter's standard `search/tweets` API using rtweet",
    x = NULL, y = NULL)


```

# Sentiment analysis

Again, another analysis I've done in the past, but here I'll use Mike Kearney's version simply to make my life a bit easier as it's mapping on to rtweet's data, and I haven't actually used the syuzhet, which is a commonly used sentiment analysis package.

```{r sentiment}
## clean up the text a bit (rm mentions and links)
rt$text2 <- gsub(
  "^RT:?\\s{0,}|#|@\\S+|https?[[:graph:]]+", "", rt$text)
## convert to lower case
rt$text2 <- tolower(rt$text2)
## trim extra white space
rt$text2 <- gsub("^\\s{1,}|\\s{1,}$", "", rt$text2)
rt$text2 <- gsub("\\s{2,}", " ", rt$text2)

## estimate pos/neg sentiment for each tweet
rt$sentiment <- syuzhet::get_sentiment(rt$text2, "syuzhet")

## write function to round time into rounded var
round_time <- function(x, sec) {
  as.POSIXct(hms::hms(as.numeric(x) %/% sec * sec))
}

## plot by specified time interval (1-hours)
rt %>%
  mutate(time = round_time(created_at, 60 * 60)) %>%
  group_by(time) %>%
  summarise(sentiment = mean(sentiment, na.rm = TRUE)) %>%
  mutate(valence = ifelse(sentiment > 0L, "Positive", "Negative")) %>%
  ggplot(aes(x = time, y = sentiment)) +
  geom_smooth(method = "loess", span = .1,
    colour = "#aa11aadd", fill = "#bbbbbb11") +
  geom_point(aes(fill = valence, colour = valence), 
    shape = 21, alpha = .6, size = 3.5) +
  theme(legend.position = "none",
        text=element_text(family='Roboto Condensed'),
    axis.text = element_text(colour = "#222222"),
    plot.title = element_text(size = rel(1.7), face = "bold"),
    plot.subtitle = element_text(size = rel(1.3)),
    plot.caption = element_text(colour = "#444444")) +
  scale_fill_manual(
    values = c(Positive = "#2244ee", Negative = "#dd2222")) +
  scale_colour_manual(
    values = c(Positive = "#001155", Negative = "#550000")) +
  labs(x = NULL, y = NULL,
    title = "Sentiment (valence) of ESTRO37 tweets over time",
    subtitle = "Mean sentiment of tweets aggregated in one-hour intervals",
    caption = "\nSource: Data gathered using rtweet. Sentiment analysis done using syuzhet")
```

# Tweet busters

So... Who are the top ranking tweeps currently?

```{r toptw}

showvals=rt %>% select(favorite_count,retweet_count,screen_name,name) %>%
  group_by(screen_name,name) %>%
  summarise(fav_count=sum(favorite_count),
            rt_count=sum(retweet_count),
            n=n()) %>% arrange(-n)

knitr::kable(showvals[1:40,])

# Includes both tweets and rtweets
showvals[1:40,]  %>%
  transform(screen_name = reorder(screen_name, n)) %>% 
  ggplot(aes(screen_name, n))+ geom_bar(stat = "identity") + 
  coord_flip() +
  theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  labs(x=NULL,y=NULL,
       title="Top 40 tweeters of ESTRO37")

showvals=rt %>% filter(is_retweet==FALSE) %>%
  select(favorite_count,retweet_count,screen_name,name) %>%
  group_by(screen_name,name) %>%
  summarise(fav_count=sum(favorite_count),
            rt_count=sum(retweet_count),
            n=n()) %>% arrange(-n)

showvals[1:40,] %>%
 transform(screen_name = reorder(screen_name, n)) %>% 
  ggplot(aes(screen_name, n))+ geom_bar(stat = "identity") + 
  coord_flip() +
  theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  labs(x=NULL,y=NULL,
       title="Top 40 retweeters of ESTRO37")
```

# Influence (dubious)

Here is that somewhat (very) dumb influence metric I cooked up or adapted from elsewhere, I can't quite remember at this point. Either way I don't put much value in it. It's basically just the sum of favorites and retweets

```{r influence}


library(viridis)
showvals2= showvals %>% mutate(impact = fav_count + rt_count) %>%
  arrange(-impact)

showvals2[1:40,] %>%
  transform(screen_name = reorder(screen_name, impact)) %>%
  ggplot(aes(screen_name, impact, fill = impact / n)) +
  geom_bar(stat = "identity") +
  coord_flip()+ ylab('Impact (numFavorites + numRetweets)') +
  theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  scale_fill_viridis(trans = "log", breaks = c(1, 5, 10, 50))


```
# Still using the hated word cloud 

The word cloud gets a bad rap, I mean, it IS basically impossible to intrepret in any numerical or comparative sense. But I do still find it useful to get a quick overview of just what types of things people are talking about. And thus, wordclouds we go.


```{r wordcloud}

library(tidytext)
library(RColorBrewer)

tidy_df = rt %>% unnest_tokens(word,text2)

tw_stop<-data.frame(word=c("ESTRO37","estro37","rtt","n","24","30","1300" ,lexicon='whatevs'))
stop_words=filter(stopwordslangs,(lang=='en' | lang=="es")  & p >.9999) %>% pull(word)
stop_words=tibble(word=stop_words)

tidy_cloud <- tidy_df %>%
 anti_join(tw_stop) %>%
  anti_join(stop_words)


hm=tidy_cloud %>%
 count(word)
library(wordcloud)
wordcloud(hm$word, hm$n,max.words = 150,colors=brewer.pal(8,'Dark2'),random.order=FALSE,random.color=FALSE)
 
```


# Finally, the network

OK, after all of the, what you actually really cared about was the network, right?

```{r network,fig.height=10,fig.width=13}


 library(igraph) 
library(hrbrthemes)
  
m_g <- rt %>%
select(screen_name, mentions_screen_name) %>%
  
  unnest(mentions_screen_name) %>% 
  
  filter(!is.na(mentions_screen_name)) %>%
  
  group_by(screen_name,mentions_screen_name) %>%
  summarise(weight=n()) %>%
  
  graph_from_data_frame()

library(igraph)
library(tidygraph)
library(ggraph)

  
  

m_graph <- m_g
dfv=data.frame(V=as.vector(V(m_graph)),screen_name=V(m_graph)$name,degree(m_graph))
names(dfv)[3]="degree"
dfv=cbind(dfv,quantile=cut(dfv$degree,breaks=quantile(dfv$degree,probs=c(0,.95,1)),labels=c("Bottom99",'Top1'),include.lowest=T))
dfv$quantile=as.character(dfv$quantile)
library(dplyr)
dfv2=arrange(dfv,desc(quantile))
dfv3=dfv2[dfv2$quantile=='Top1',]
print(nrow(dfv3))

red_gr=induced_subgraph(m_graph,dfv3$V)

#g=as_tbl_graph(red_gr) %>%
#  mutate(pop=centrality_pagerank())

#ggraph(g,layout='kk')+
#  geom_edge_fan(aes(alpha=..index..),show.legend=FALSE) +
#  geom_node_point(aes(size=pop),show.legend=FALSE) +geom_node_label(aes(label=name))+theme_graph()


V(red_gr)$node_label <- unname(ifelse(degree(red_gr)[V(red_gr)] > 20, names(V(red_gr)), "")) 

V(red_gr)$node_size <- unname(ifelse(degree(red_gr)[V(red_gr)] > 20, degree(red_gr), 0)) 



ggraph(red_gr, layout = 'linear', circular = TRUE) + 
  
  geom_edge_arc(edge_width=0.125, aes(alpha=..index..)) +
  
  geom_node_label(aes(label=node_label, size=node_size),
                  
                  label.size=0, fill="#ffffff66", segment.colour="springgreen",
                  
                  color="slateblue", repel=TRUE, family=font_rc, fontface="bold") +
  
  coord_fixed() +
  
  scale_size_area(trans="sqrt") +
  
  labs(title="Mention Relationships", subtitle="Most mentioned screen names labeled. Darkers edges == more retweets. Node size == larger degree") +
  
  theme_graph(base_family=font_rc) +
  
  theme(legend.position="none")


# retweet analysis

rt_g=filter(rt, retweet_count > 0) %>% 
  
  select(screen_name, retweet_screen_name) %>%
  
  filter(!is.na(retweet_screen_name)) %>% 
  
  graph_from_data_frame() 


dfv=data.frame(V=as.vector(V(rt_g)),screen_name=V(rt_g)$name,degree(rt_g))
names(dfv)[3]="degree"
dfv=cbind(dfv,quantile=cut(dfv$degree,breaks=quantile(dfv$degree,probs=c(0,.9,1)),labels=c("Bottom99",'Top1'),include.lowest=T))
dfv$quantile=as.character(dfv$quantile)
library(dplyr)
dfv2=arrange(dfv,desc(quantile))
dfv3=dfv2[dfv2$quantile=='Top1',]
print(nrow(dfv3))




ndf <- rt %>% filter(screen_name %in% dfv3$screen_name)
nrow(ndf %>% filter(!is.na(retweet_screen_name)))
nrow(ndf %>% filter(is.na(retweet_screen_name)))

red_gr_rt=induced_subgraph(rt_g,dfv3$V)



V(red_gr_rt)$node_label <- unname(ifelse(degree(red_gr_rt)[V(red_gr_rt)] > 25, names(V(red_gr_rt)), "")) 

V(red_gr_rt)$node_size <- unname(ifelse(degree(red_gr_rt)[V(red_gr_rt)] > 25, degree(red_gr_rt), 0)) 



ggraph(red_gr_rt, layout = 'linear', circular = TRUE) + 
  
  geom_edge_arc(edge_width=0.125, aes(alpha=..index..)) +
  
  geom_node_label(aes(label=node_label, size=node_size),
                  
                  label.size=0, fill="#ffffff66", segment.colour="springgreen",
                  
                  color="slateblue", repel=TRUE, family=font_rc, fontface="bold") +
  
  coord_fixed() +
  
  scale_size_area(trans="sqrt") +
  
  labs(title="Retweet Relationships", subtitle="Most retweeted screen names labeled. Darkers edges == more retweets. Node size == larger degree") +
  
  theme_graph(base_family=font_rc) +
  
  theme(legend.position="none")

  
  
 
```